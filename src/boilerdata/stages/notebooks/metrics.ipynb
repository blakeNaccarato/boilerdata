{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\"\"\"Toggle this to run exploratory cells in the notebook.\"\"\"\n",
    "display()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# # This check is flaky in a notebook\n",
    "# pyright: reportUnnecessaryTypeIgnoreComment=none\n",
    "from contextlib import contextmanager\n",
    "import json\n",
    "from pathlib import Path\n",
    "from shutil import copy\n",
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import janitor  # pyright: ignore [reportUnusedImport]  # adds methods to pd.DataFrame\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from uncertainties import ufloat\n",
    "\n",
    "from boilerdata.axes_enum import AxesEnum as A  # noqa: N814\n",
    "from boilerdata.models.enums import Joint\n",
    "from boilerdata.models.project import Project\n",
    "from boilerdata.stages.common import get_tcs, get_trial, per_run\n",
    "from boilerdata.stages.notebooks.common import FLOAT_SPEC  # type: ignore  # magic\n",
    "from boilerdata.stages.notebooks.common import (\n",
    "    add_units,\n",
    "    chdir_to_nearest_git_root_and_get_project,\n",
    "    tex_wrap,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook and plot formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%precision %$FLOAT_SPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "proj = chdir_to_nearest_git_root_and_get_project()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "meta = [col.name for col in proj.axes.meta]\n",
    "errors = proj.params.free_errors\n",
    "fits = proj.params.free_params\n",
    "df_in = pd.read_csv(\n",
    "    proj.dirs.file_results,\n",
    "    index_col=(index := [A.trial, A.run]),\n",
    "    parse_dates=index,\n",
    "    dtype={col.name: col.dtype for col in proj.axes.cols},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This warning fires unnecessarily when Seaborn or Pandas plots are placed in existing\n",
    "axes. This warning can't be caught in context of `warnings.catch_warnings()` because\n",
    "it fires *after* a cell finishes executing. So we have to disable this globally.\"\"\"\n",
    "warnings.filterwarnings(\n",
    "    category=UserWarning,\n",
    "    action=\"ignore\",\n",
    "    message=\"This figure includes Axes that are not compatible with\",\n",
    ")\n",
    "\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\", palette=\"deep\", font=\"sans-serif\")\n",
    "plt.style.use(style=proj.dirs.file_style)\n",
    "\n",
    "cmap_blues = mpl.colormaps[\"Blues\"]  # type: ignore  # matplotlib\n",
    "cmap_reds = mpl.colormaps[\"Reds\"]  # type: ignore  # matplotlib\n",
    "\n",
    "display(cmap_blues, cmap_reds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def manual_subplot_spacing():\n",
    "    \"\"\"Context manager that allows custom spacing of subplots.\"\"\"\n",
    "    with mpl.rc_context({\"figure.autolayout\": False}):\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            ...\n",
    "\n",
    "\n",
    "idxs = pd.IndexSlice\n",
    "\"\"\"Use to slice pd.MultiIndex indices\"\"\"\n",
    "\n",
    "\n",
    "def display_named(*args: tuple[Any, str]):\n",
    "    \"\"\"Display objects with names above them.\"\"\"\n",
    "    for elem, name in args:\n",
    "        display(Markdown(f\"##### {name}\"))\n",
    "        display(elem)\n",
    "        print()\n",
    "\n",
    "\n",
    "def get_default_aggs(columns: list[str]) -> dict[str, pd.NamedAgg]:\n",
    "    \"\"\"Get default aggregations for columns that are part of the original dataset.\"\"\"\n",
    "    return {col: agg for col, agg in proj.axes.aggs.items() if col in columns}\n",
    "\n",
    "\n",
    "def get_params_mapping_with_uncertainties(\n",
    "    grp: pd.DataFrame, proj: Project\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Get a mapping of parameter names to values with uncertainty.\"\"\"\n",
    "    model_params_and_errors = proj.params.params_and_errors\n",
    "    # Reason: pydantic: use_enum_values\n",
    "    params: list[str] = proj.params.model_params  # type: ignore\n",
    "    param_errors: list[str] = proj.params.model_errors\n",
    "    u_params = [\n",
    "        ufloat(param, err, tag)\n",
    "        for param, err, tag in zip(\n",
    "            grp[params], grp[param_errors], model_params_and_errors\n",
    "        )\n",
    "    ]\n",
    "    return dict(zip(model_params_and_errors, u_params))\n",
    "\n",
    "\n",
    "def get_params_mapping(grp: pd.DataFrame, params: list[Any]) -> dict[str, Any]:\n",
    "    \"\"\"Get a mapping of parameter names to values.\"\"\"\n",
    "    # Reason: pydantic: use_enum_values\n",
    "    return dict(zip(params, grp[params]))\n",
    "\n",
    "\n",
    "def model_with_error(model, x, u_params):\n",
    "    \"\"\"Evaluate the model for x and return y with errors.\"\"\"\n",
    "    u_x = [ufloat(v, 0, \"x\") for v in x]\n",
    "    u_y = model(u_x, **u_params)\n",
    "    y = np.array([v.nominal_value for v in u_y])\n",
    "    y_min = y - [\n",
    "        v.std_dev for v in u_y\n",
    "    ]  # pyright: ignore [reportGeneralTypeIssues]  # uncertainties, triggered only locally\n",
    "    y_max = y + [v.std_dev for v in u_y]\n",
    "    return y, y_min, y_max"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error distribution by joint type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df = df_in[[A.joint, *errors]]  # .pipe(add_units, proj)\n",
    "display_named((df.groupby(A.joint).max(), \"Max error by joint type\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show errors by joint type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "for agg, path in dict(\n",
    "    median=proj.dirs.plot_median_error_by_joint,\n",
    "    max=proj.dirs.plot_max_error_by_joint,\n",
    ").items():\n",
    "    fig, ax = plt.subplots()\n",
    "    df = df_in[[A.joint, *errors]]\n",
    "    err = df.groupby(A.joint).agg(agg)\n",
    "    err_scaled = err.div(pd.Series(proj.params.error_scale)[errors])\n",
    "    sns.heatmap(\n",
    "        ax=ax,\n",
    "        data=err_scaled.pipe(tex_wrap),\n",
    "        annot=err,\n",
    "        cmap=cmap_blues,\n",
    "        square=True,\n",
    "    )\n",
    "    ax.set_title(f\"{agg.title()} Error\")\n",
    "    ax.set_xlabel(\"Error\")\n",
    "    ax.set_ylabel(\"Joint Type\")\n",
    "    fig.savefig(path)  # type: ignore  # matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show errors by temperature range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "temp_range = \"Temperature Range\"\n",
    "for agg, path in dict(\n",
    "    median=proj.dirs.plot_median_error_by_range,\n",
    "    max=proj.dirs.plot_max_error_by_range,\n",
    ").items():\n",
    "    fig, ax = plt.subplots()\n",
    "    columns = [A.T_5, *errors]\n",
    "    temperature_bin_left_edges = dict(\n",
    "        Under=-np.inf,\n",
    "        Low=100,\n",
    "        Med=110,\n",
    "        High=120,\n",
    "        Over=130,\n",
    "    )\n",
    "    temperature_bin_edges = list(temperature_bin_left_edges.values()) + [np.inf]\n",
    "    labels = list(temperature_bin_left_edges.keys())\n",
    "    df = (\n",
    "        df_in[columns]\n",
    "        .assign(\n",
    "            **{\n",
    "                temp_range: lambda df: pd.cut(\n",
    "                    df[A.T_5],\n",
    "                    bins=temperature_bin_edges,\n",
    "                    labels=labels,\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        .groupby(temp_range)\n",
    "        .agg(agg)\n",
    "        .drop(axis=\"columns\", labels=A.T_5)\n",
    "    )\n",
    "    err = df\n",
    "    err_scaled = err.div(pd.Series(proj.params.error_scale)[errors])\n",
    "\n",
    "    sns.heatmap(\n",
    "        ax=ax,\n",
    "        data=err_scaled.pipe(tex_wrap),\n",
    "        annot=err,\n",
    "        cmap=cmap_reds,\n",
    "        square=True,\n",
    "    )\n",
    "    ax.set_title(f\"{agg.title()} Error\")\n",
    "    ax.set_xlabel(\"Error\")\n",
    "    fig.savefig(path)  # type: ignore  # matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the T5 distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [A.T_5]\n",
    "xmin = 100\n",
    "xmax = 270\n",
    "ymin = 0\n",
    "ymax = 1\n",
    "quantiles = (0.00, 0.20, 0.55, 0.90, 1.00)\n",
    "labels = (\"Under\", \"Low\", \"High\", \"Over\")\n",
    "\n",
    "if DEBUG:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    _ = sns.kdeplot(\n",
    "        ax=ax,\n",
    "        data=df_in[cols],\n",
    "        cumulative=True,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"Cumulative KDE of T5\")\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_autoscale_on(False)\n",
    "    _ = ax.hlines(quantiles, xmin, xmax, colors=[0.5] * 3)  # type: ignore  # matplotlib\n",
    "    ax.add_patch(\n",
    "        mpl.patches.Rectangle(  # type: ignore  # matplotlib\n",
    "            xy=(xmin, quantiles[0]),\n",
    "            width=xmax - xmin,\n",
    "            height=quantiles[1] - quantiles[0],\n",
    "            color=\"red\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    )\n",
    "    ax.add_patch(\n",
    "        mpl.patches.Rectangle(  # type: ignore  # matplotlib\n",
    "            xy=(xmin, quantiles[-1]),\n",
    "            width=xmax - xmin,\n",
    "            height=quantiles[-2] - quantiles[-1],\n",
    "            color=\"red\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin the data by useful T5 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.qcut(df_in[A.T_5], quantiles).cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [75.756, 101.476, 117.001, 161.608, 264.268]\n",
    "\"\"\"Determined from the exploration above and fixed.\"\"\"\n",
    "\n",
    "cols = [A.joint, A.T_5, *errors]\n",
    "temp_range = \"Temperature Range\"\n",
    "df = (\n",
    "    df_in[cols]\n",
    "    .assign(\n",
    "        **{\n",
    "            temp_range: lambda df: (\n",
    "                pd.cut(\n",
    "                    x=df_in[A.T_5],\n",
    "                    bins=bins,\n",
    "                    labels=labels,\n",
    "                    right=True,\n",
    "                )\n",
    "            ).cat.remove_categories([\"Under\", \"Over\"]),\n",
    "        },\n",
    "    )\n",
    "    .dropna(axis=\"index\", subset=[temp_range])\n",
    "    .drop(axis=\"columns\", labels=A.T_5)\n",
    ")\n",
    "display_named(\n",
    "    (df[temp_range].value_counts().to_frame(), \"Counts\"),\n",
    "    (pd.DataFrame(index=df.columns), \"Columns\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for error in errors:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(\n",
    "        ax=ax,\n",
    "        data=df,\n",
    "        x=error,\n",
    "        y=A.joint,\n",
    "        hue=temp_range,\n",
    "    )\n",
    "    sns.move_legend(obj=ax, loc=\"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{err: scale for err, scale in free_error_scale.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_error_scale = {\n",
    "    k: v for k, v in proj.params.error_scale.items() if k in proj.params.free_errors  # type: ignore  # pydantic: use_enum_values\n",
    "}\n",
    "\n",
    "df_scaled = df.assign(\n",
    "    **{err: lambda df: df[err].div(scale) for err, scale in free_error_scale.items()}\n",
    ")\n",
    "\n",
    "for error in errors:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(\n",
    "        data=df_scaled,\n",
    "        x=error,\n",
    "        y=A.joint,\n",
    "        hue=temp_range,\n",
    "    )\n",
    "    sns.move_legend(obj=ax, loc=\"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "def write_metrics(df: pd.DataFrame, proj: Project, errors, fits):\n",
    "    \"\"\"Compute summary metrics of the model fit and write them to a file.\"\"\"\n",
    "    # sourcery skip: merge-dict-assign\n",
    "    first_fit = fits[0]\n",
    "\n",
    "    def strip_err(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Strip the \"err\" suffix from the column names.\"\"\"\n",
    "        return df.rename(axis=\"columns\", mapper=lambda col: col.removesuffix(\"_err\"))\n",
    "\n",
    "    # Reason: pydantic: use_enum_values\n",
    "    error_ratio = df[errors].pipe(strip_err) / df[fits]\n",
    "    error_normalized = (df[errors] / df[errors].max()).pipe(strip_err)\n",
    "\n",
    "    # Compute the rate of failures to fit the model\n",
    "    metrics: dict[str, float] = {}\n",
    "    metrics[\"fit_failure_rate\"] = df[first_fit].isna().sum() / len(df)\n",
    "\n",
    "    # Compute the median and spread of the error two ways\n",
    "    metric_dfs = {\"err_ratio\": error_ratio, \"err_norm\": error_normalized}\n",
    "    for err_tag, err_df in metric_dfs.items():\n",
    "        for agg in [\"median\", \"std\"]:\n",
    "            metrics |= {\n",
    "                f\"{k}_{err_tag}_{agg}\": v for k, v in err_df.agg(agg).to_dict().items()\n",
    "            }\n",
    "    metrics |= {k: 0 for k, v in metrics.items() if np.isnan(v)}\n",
    "    proj.dirs.file_pipeline_metrics.write_text(json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "def plot_new_fits(grp: pd.DataFrame, proj: Project, model):\n",
    "    \"\"\"Plot model fits for trials marked as new.\"\"\"\n",
    "\n",
    "    trial = get_trial(grp, proj)\n",
    "    if not trial.new:\n",
    "        return grp\n",
    "\n",
    "    ser = grp.squeeze()\n",
    "    tcs, tc_errors = get_tcs(trial)\n",
    "    x_unique = list(trial.thermocouple_pos.values())\n",
    "    y_unique = ser[tcs]\n",
    "\n",
    "    # Plot setup\n",
    "    fig, ax = plt.subplots(layout=\"constrained\")\n",
    "\n",
    "    run = ser.name[-1].isoformat()\n",
    "    run_file = proj.dirs.new_fits / f\"{run.replace(':', '-')}.png\"\n",
    "\n",
    "    ax.margins(0, 0)\n",
    "    ax.set_title(f\"{run = }\")\n",
    "    ax.set_xlabel(\"x (m)\")\n",
    "    ax.set_ylabel(\"T (C)\")\n",
    "\n",
    "    # Initial plot boundaries\n",
    "    x_bounds = np.array([0, trial.thermocouple_pos[A.T_1]])\n",
    "\n",
    "    y_bounds = model(x_bounds, **get_params_mapping(ser, proj.params.model_params))\n",
    "    ax.plot(\n",
    "        x_bounds,\n",
    "        y_bounds,\n",
    "        \"none\",\n",
    "    )\n",
    "\n",
    "    # Measurements\n",
    "    measurements_color = [0.2, 0.2, 0.2]\n",
    "    ax.plot(\n",
    "        x_unique,\n",
    "        y_unique,\n",
    "        \".\",\n",
    "        label=\"Measurements\",\n",
    "        color=measurements_color,\n",
    "        markersize=10,\n",
    "    )\n",
    "    ax.errorbar(\n",
    "        x=x_unique,\n",
    "        y=y_unique,\n",
    "        yerr=ser[tc_errors],\n",
    "        fmt=\"none\",\n",
    "        color=measurements_color,\n",
    "    )\n",
    "\n",
    "    # Confidence interval\n",
    "    (xlim_min, xlim_max) = ax.get_xlim()\n",
    "    pad = 0.025 * (xlim_max - xlim_min)\n",
    "    x_padded = np.linspace(xlim_min - pad, xlim_max + pad)\n",
    "\n",
    "    y_padded, y_padded_min, y_padded_max = model_with_error(\n",
    "        model, x_padded, get_params_mapping_with_uncertainties(ser, proj)\n",
    "    )\n",
    "    ax.plot(\n",
    "        x_padded,\n",
    "        y_padded,\n",
    "        \"--\",\n",
    "        label=\"Model Fit\",\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        x=x_padded,\n",
    "        y1=y_padded_min,\n",
    "        y2=y_padded_max,  # pyright: ignore [reportGeneralTypeIssues]  # matplotlib\n",
    "        color=[0.8, 0.8, 0.8],\n",
    "        edgecolor=[1, 1, 1],\n",
    "        label=\"95% CI\",\n",
    "    )\n",
    "\n",
    "    # Extrapolation\n",
    "    ax.plot(\n",
    "        0,\n",
    "        ser[A.T_s],\n",
    "        \"x\",\n",
    "        label=\"Extrapolation\",\n",
    "        color=[1, 0, 0],\n",
    "    )\n",
    "\n",
    "    # Finishing\n",
    "    ax.legend()\n",
    "    fig.savefig(\n",
    "        run_file,  # pyright: ignore [reportGeneralTypeIssues]  # matplotlib\n",
    "        dpi=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "def plot_fits(df: pd.DataFrame, proj: Project, model) -> pd.DataFrame:\n",
    "    \"\"\"Get the latest new model fit plot.\"\"\"\n",
    "    if proj.params.do_plot:\n",
    "        per_run(df, plot_new_fits, proj, model)\n",
    "        if figs_src := sorted(proj.dirs.new_fits.iterdir()):\n",
    "            figs_src = (\n",
    "                figs_src[0],\n",
    "                figs_src[len(figs_src) // 2],\n",
    "                figs_src[-1],\n",
    "            )\n",
    "            figs_dst = (\n",
    "                proj.dirs.plot_new_fit_0,\n",
    "                proj.dirs.plot_new_fit_1,\n",
    "                proj.dirs.plot_new_fit_2,\n",
    "            )\n",
    "            for fig_src, fig_dst in zip(figs_src, figs_dst):\n",
    "                copy(fig_src, fig_dst)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "3"
    }
   },
   "outputs": [],
   "source": [
    "from boilerdata.stages.modelfun import model_with_uncertainty\n",
    "\n",
    "_ = df_in.also(write_metrics, proj, errors, fits).also(\n",
    "    plot_fits, proj, model_with_uncertainty\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6aa036b71bee0863c84db770605bec9f16973f1b95b8b091417c7a1242fbf62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
